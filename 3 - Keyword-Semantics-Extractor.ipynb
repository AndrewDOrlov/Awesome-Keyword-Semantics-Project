{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40f794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4beac60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Загрузка датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d523ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SecurityLab = pd.read_csv(\"SecurityLab.csv\", encoding = \"UTF-8\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554a9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(SecurityLab[\"Tags\"])\n",
    "texts = list(SecurityLab[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd46131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nlp-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7e55036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nlp_rake import Rake\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stops = list(set(stopwords.words(\"russian\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2083b1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('злоумышленники получают доступ', 9.0), ('открытым исходным кодом', 9.0), ('принудительное завершение работы', 9.0), ('совершать боковое перемещение', 9.0), ('двоичные файлы вредоносного', 9.0)]\n"
     ]
    }
   ],
   "source": [
    "rake = Rake(stopwords = stops, max_words = 3)\n",
    "\n",
    "keywords = rake.apply(texts[0])\n",
    "\n",
    "print(keywords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef706362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2da225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6aeb9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = yake.KeywordExtractor(\n",
    "lan= \"ru\",\n",
    "n = 2,\n",
    "dedupLim = 0.3,\n",
    "top = 10\n",
    ")\n",
    "\n",
    "yakelist=[]\n",
    "\n",
    "for i in texts:\n",
    "    for a in extractor.extract_keywords(i):\n",
    "        yakelist.append(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb68d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yakelist=[]\n",
    "\n",
    "for i in extractor.extract_keywords(texts[0]):\n",
    "    yakelist.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e9addcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Восточной Азии',\n",
       " 'Исследователи кибербезопасности',\n",
       " 'SentinelLabs сообщают',\n",
       " 'кибербезопасности SentinelLabs',\n",
       " 'китайскоязычная группировка',\n",
       " 'проведении шпионских',\n",
       " 'шпионских атак',\n",
       " 'кода Golang',\n",
       " 'Азии',\n",
       " 'исходного кода',\n",
       " 'млн долларов',\n",
       " 'краденных активов',\n",
       " 'взлом блокчейн-моста',\n",
       " 'оборот краденных',\n",
       " 'млн',\n",
       " 'staked ETH',\n",
       " 'Wormhole',\n",
       " 'Lido Finance',\n",
       " 'начал оборот',\n",
       " 'платформа',\n",
       " 'января китайские',\n",
       " 'пользователи обнаружили',\n",
       " 'продаются секретные',\n",
       " 'тайваньских спецслужб',\n",
       " 'китайские пользователи',\n",
       " 'Breach Forums',\n",
       " 'секретные документы',\n",
       " 'документы тайваньских',\n",
       " 'разведки',\n",
       " 'данных',\n",
       " 'Riot Games',\n",
       " 'письмо фанатам',\n",
       " 'прошлой неделе',\n",
       " 'опубликовала открытое',\n",
       " 'открытое письмо',\n",
       " 'Teamfight Tactics',\n",
       " 'Games опубликовала',\n",
       " 'League',\n",
       " 'Legends',\n",
       " 'исходный код',\n",
       " 'привлечь обратно',\n",
       " 'создавать благоприятные',\n",
       " 'Дмитрий Песков',\n",
       " 'благоприятные условия',\n",
       " 'уехавших IT-специалистов',\n",
       " 'пресс-секретарь президента',\n",
       " 'необходимо создавать',\n",
       " 'страну уехавших',\n",
       " 'России',\n",
       " 'Владимира Потанина',\n",
       " 'Динамику перехода',\n",
       " 'решения Достаточность',\n",
       " 'Достаточность функционала',\n",
       " 'проводим исследование',\n",
       " 'востребованных вендоров',\n",
       " 'альтернативные решения',\n",
       " 'платформ виртуализации',\n",
       " 'сфере платформ',\n",
       " 'Стек Групп',\n",
       " 'оценить',\n",
       " 'рассматривает возможность',\n",
       " 'возможность введения',\n",
       " 'введения новых',\n",
       " 'новых требований',\n",
       " 'основе открытого',\n",
       " 'Минцифры России',\n",
       " 'России рассматривает',\n",
       " 'реестр',\n",
       " 'отечественного софта',\n",
       " 'СПО',\n",
       " 'XLL',\n",
       " 'добавлением защиты',\n",
       " 'Microsoft',\n",
       " 'Надстройки Excel',\n",
       " 'Интернета',\n",
       " 'растущим числом',\n",
       " 'защиты надстроек',\n",
       " 'Office',\n",
       " 'файлы',\n",
       " 'вредоносных',\n",
       " 'Управление операциями',\n",
       " 'курсу MBA',\n",
       " 'экзамена',\n",
       " 'студенты бизнес-колледжей',\n",
       " 'успешно сдал',\n",
       " 'ChatGPT успешно',\n",
       " 'США',\n",
       " 'медицинской лицензии',\n",
       " 'говорится',\n",
       " 'Ранее',\n",
       " 'нулевого дня',\n",
       " 'устранена компанией',\n",
       " 'вышедшими обновлениями',\n",
       " 'компанией Apple',\n",
       " 'iPhone',\n",
       " 'безопасности',\n",
       " 'Уязвимость нулевого',\n",
       " 'дня',\n",
       " 'декабре',\n",
       " 'iPad Air']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yakelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2607a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy match function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01da0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8820d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download xx_sent_ud_sm\n",
    "#! python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb82334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Восточной Азии\n",
      "Восточной ADJ amod\n",
      "Азии PROPN ROOT\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.ru.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(yakelist[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2f42272",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Восточной Азии\n",
      "Восточной ADJ Case=Gen|Degree=Pos|Gender=Fem|Number=Sing amod\n",
      "Азии PROPN Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing ROOT\n",
      "Исследователи кибербезопасности\n",
      "Исследователи NOUN Animacy=Anim|Case=Nom|Gender=Masc|Number=Plur ROOT\n",
      "кибербезопасности NOUN Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing nmod\n",
      "SentinelLabs сообщают\n",
      "SentinelLabs PROPN Foreign=Yes nsubj\n",
      "сообщают VERB Aspect=Imp|Mood=Ind|Number=Plur|Person=Third|Tense=Pres|VerbForm=Fin|Voice=Act ROOT\n",
      "кибербезопасности SentinelLabs\n",
      "кибербезопасности NOUN Animacy=Inan|Case=Nom|Gender=Fem|Number=Plur ROOT\n",
      "SentinelLabs PROPN Foreign=Yes nmod\n",
      "китайскоязычная группировка\n",
      "китайскоязычная ADJ Case=Nom|Degree=Pos|Gender=Fem|Number=Sing amod\n",
      "группировка NOUN Animacy=Inan|Case=Nom|Gender=Fem|Number=Sing ROOT\n",
      "проведении шпионских\n",
      "проведении NOUN Animacy=Inan|Case=Loc|Gender=Neut|Number=Sing ROOT\n",
      "шпионских ADJ Case=Gen|Degree=Pos|Number=Plur nmod\n",
      "шпионских атак\n",
      "шпионских ADJ Case=Gen|Degree=Pos|Number=Plur amod\n",
      "атак NOUN Animacy=Inan|Case=Gen|Gender=Fem|Number=Plur ROOT\n",
      "кода Golang\n",
      "кода NOUN Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing ROOT\n",
      "Golang PROPN Foreign=Yes appos\n",
      "Азии\n",
      "Азии PROPN Animacy=Inan|Case=Dat|Gender=Fem|Number=Sing ROOT\n",
      "исходного кода\n",
      "исходного ADJ Case=Gen|Degree=Pos|Gender=Masc|Number=Sing amod\n",
      "кода NOUN Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing ROOT\n"
     ]
    }
   ],
   "source": [
    "for i in yakelist:\n",
    "    doc = nlp(i)\n",
    "    print(doc.text)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.morph, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07adf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "698c8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(keyword):\n",
    "    patterns = [\n",
    "        [{'POS': 'PROPN'}, {'POS': 'VERB'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],  \n",
    "        [{'POS': 'NOUN'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'ADV'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'ADP'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'ADP'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'VERB'}, {'POS': 'ADV'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'NOUN'}],\n",
    "        ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"pos-matcher\", patterns)\n",
    "    doc = nlp(keyword)\n",
    "    matches = matcher(doc)\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "788e5067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Восточной Азии\n",
      "Исследователи кибербезопасности\n",
      "китайскоязычная группировка\n",
      "шпионских атак\n",
      "исходного кода\n",
      "млн долларов\n",
      "краденных активов\n",
      "взлом блокчейн-моста\n",
      "начал оборот\n",
      "пользователи обнаружили\n",
      "тайваньских спецслужб\n",
      "китайские пользователи\n",
      "секретные документы\n",
      "письмо фанатам\n",
      "прошлой неделе\n",
      "открытое письмо\n",
      "исходный код\n",
      "привлечь обратно\n",
      "Дмитрий Песков\n",
      "благоприятные условия\n",
      "уехавших IT-специалистов\n",
      "пресс-секретарь президента\n",
      "страну уехавших\n",
      "Владимира Потанина\n",
      "Динамику перехода\n",
      "Достаточность функционала\n",
      "проводим исследование\n",
      "востребованных вендоров\n",
      "альтернативные решения\n",
      "платформ виртуализации\n",
      "сфере платформ\n",
      "Стек Групп\n",
      "рассматривает возможность\n",
      "возможность введения\n",
      "новых требований\n",
      "отечественного софта\n",
      "добавлением защиты\n",
      "растущим числом\n",
      "защиты надстроек\n",
      "Управление операциями\n",
      "студенты бизнес-колледжей\n",
      "медицинской лицензии\n",
      "нулевого дня\n",
      "устранена компанией\n",
      "вышедшими обновлениями\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in yakelist:\n",
    "    if match(i) == True:\n",
    "        print(i)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(keyword):\n",
    "    \"\"\"This function checks if a list of keywords match a certain POS pattern\"\"\"\n",
    "    patterns = [\n",
    "        [{'POS': 'PROPN'}, {'POS': 'VERB'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],  \n",
    "        [{'POS': 'NOUN'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'ADV'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'ADP'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'ADP'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'NOUN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'VERB'}, {'POS': 'ADV'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'NOUN'}],\n",
    "        ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"pos-matcher\", patterns)\n",
    "    # create spacy object\n",
    "    doc = nlp(keyword)\n",
    "    # iterate through the matches\n",
    "    matches = matcher(doc)\n",
    "    # if matches is not empty, it means that it has found at least a match\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f52971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_from_corpus(extractor, corpus):\n",
    "    \"\"\"This function uses an extractor to retrieve keywords from a list of documents\"\"\"\n",
    "    extractor_name = extractor.__name__.replace(\"_extractor\", \"\")\n",
    "    logging.info(f\"Starting keyword extraction with {extractor_name}\")\n",
    "    corpus_kws = {}\n",
    "    start = time.time()\n",
    "    # logging.info(f\"Timer initiated.\") <-- uncomment this if you want to output start of timer\n",
    "    for idx, text in tqdm(enumerate(corpus), desc=\"Extracting keywords from corpus...\"):\n",
    "        corpus_kws[idx] = extractor(text)\n",
    "    end = time.time()\n",
    "    # logging.info(f\"Timer stopped.\") <-- uncomment this if you want to output end of timer\n",
    "    elapsed = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
    "    logging.info(f\"Time elapsed: {elapsed}\")\n",
    "    \n",
    "    return {\"algorithm\": extractor.__name__, \n",
    "            \"corpus_kws\": corpus_kws, \n",
    "            \"elapsed_time\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfbe3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db486fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # create results dataframe\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"results.csv\", index=False)\n",
    "    logging.info(\"Benchmark finished. Results saved to results.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5142d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(corpus, shuffle=True):\n",
    "    \"\"\"This function runs the benchmark for the keyword extraction algorithms\"\"\"\n",
    "    logging.info(\"Starting benchmark...\\n\")\n",
    "    \n",
    "    # Shuffle the corpus\n",
    "    if shuffle:\n",
    "        random.shuffle(corpus)\n",
    "\n",
    "    # extract keywords from corpus\n",
    "    results = []\n",
    "    extractors = [\n",
    "        rake_extractor, \n",
    "        yake_extractor, \n",
    "        topic_rank_extractor, \n",
    "        position_rank_extractor,\n",
    "        single_rank_extractor,\n",
    "        multipartite_rank_extractor,\n",
    "        keybert_extractor,\n",
    "    ]\n",
    "    for extractor in extractors:\n",
    "        result = extract_keywords_from_corpus(extractor, corpus)\n",
    "        results.append(result)\n",
    "\n",
    "    # compute average number of extracted keywords\n",
    "    for result in results:\n",
    "        len_of_kw_list = []\n",
    "        for kws in result[\"corpus_kws\"].values():\n",
    "            len_of_kw_list.append(len(kws))\n",
    "        result[\"avg_keywords_per_document\"] = np.mean(len_of_kw_list)\n",
    "\n",
    "    # match keywords\n",
    "    for result in results:\n",
    "        for idx, kws in result[\"corpus_kws\"].items():\n",
    "            match_results = []\n",
    "            for kw in kws:\n",
    "                match_results.append(match(kw))\n",
    "                result[\"corpus_kws\"][idx] = match_results\n",
    "\n",
    "    # compute average number of matched keywords\n",
    "    for result in results:\n",
    "        len_of_matching_kws_list = []\n",
    "        for idx, kws in result[\"corpus_kws\"].items():\n",
    "            len_of_matching_kws_list.append(len([kw for kw in kws if kw]))\n",
    "        result[\"avg_matched_keywords_per_document\"] = np.mean(len_of_matching_kws_list)\n",
    "        # compute average percentange of matching keywords, round 2 decimals\n",
    "        result[\"avg_percentage_matched_keywords\"] = round(result[\"avg_matched_keywords_per_document\"] / result[\"avg_keywords_per_document\"], 2)\n",
    "        \n",
    "    # create score based on the avg percentage of matched keywords divided by time elapsed (in seconds)\n",
    "    for result in results:\n",
    "        elapsed_seconds = get_sec(result[\"elapsed_time\"]) + 0.1\n",
    "        # weigh the score based on the time elapsed\n",
    "        result[\"performance_score\"] = round(result[\"avg_matched_keywords_per_document\"] / elapsed_seconds, 2)\n",
    "    \n",
    "    # delete corpus_kw\n",
    "    for result in results:\n",
    "        del result[\"corpus_kws\"]\n",
    "\n",
    "    # create results dataframe\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"results.csv\", index=False)\n",
    "    logging.info(\"Benchmark finished. Results saved to results.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad98d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cee2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4da486dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "wget(\"https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f084a7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [.......................................] 53012480 / 53012480"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Navec1.tar'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download(\"https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\", \"Navec1.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf99d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "path = 'data/navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    "#path = 'data/navec_hudlit_v1_12B_500K_300d_100q.tar' uncomment if wget is used\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa3ea230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf070e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Navec1.tar\"\n",
    "#path = 'data/navec_hudlit_v1_12B_500K_300d_100q.tar' uncomment if wget is used\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d04a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3955571 ,  0.11600914,  0.24605067, -0.35206917, -0.08932345,\n",
       "        0.3382279 , -0.5457616 ,  0.07472657, -0.4753835 , -0.3330848 ,\n",
       "        0.1449912 , -0.13690177,  0.0840969 , -0.2141802 , -0.57312167,\n",
       "       -0.06113251,  0.04558022, -0.30147526,  0.55966735, -0.30724582,\n",
       "       -0.23867525, -0.28796813, -0.10781458,  0.09957517,  0.25810853,\n",
       "        0.05649512, -0.03052731,  0.06966446,  0.01507293, -0.14274776,\n",
       "       -0.40885502,  0.2893703 ,  0.3201632 , -0.27072856, -0.04702847,\n",
       "        0.41056094,  0.51552117, -0.27452162, -0.10154261,  0.41732824,\n",
       "        0.15827584, -0.03746929,  0.17745009, -0.03559239, -0.29721352,\n",
       "       -0.39473224, -0.04143994,  0.05495927,  0.47845373, -0.32670122,\n",
       "        0.14475909,  0.35896015, -0.3800541 ,  0.29924598, -0.31741822,\n",
       "       -0.71888554, -0.35691768, -0.2958643 , -0.37184098,  0.08903536,\n",
       "       -0.12709911, -0.06632375,  0.03680592,  0.27049237,  0.00893382,\n",
       "       -0.20365159, -0.27380955,  0.08020782,  0.12610178,  0.04507966,\n",
       "       -0.56111956, -0.62939566, -0.32951435, -0.1810639 , -0.28960884,\n",
       "        0.6197792 , -0.10272317, -0.12893517,  0.2511102 , -0.04666989,\n",
       "       -0.01017368,  0.2778579 , -0.06260373,  0.32465425,  0.61492896,\n",
       "        0.13165517,  0.2256508 ,  0.4113643 , -0.07210559,  0.21978192,\n",
       "        0.2030308 , -0.4252658 , -0.03872509, -0.3849392 , -0.17426725,\n",
       "        0.65441537, -0.54959536, -0.41553697,  0.03038502,  0.63821596,\n",
       "        0.02551245, -0.10792938,  0.4714141 ,  0.17638063,  0.21021639,\n",
       "       -0.22764762, -0.07045569, -0.27518165,  0.1774276 ,  0.23115063,\n",
       "       -0.59743494,  0.44871888, -0.10782384,  0.37884295,  0.29360834,\n",
       "        0.0363479 ,  0.22075066, -0.34550273, -0.2987647 , -0.14842781,\n",
       "       -0.23698306, -0.14348198, -0.0511187 , -0.14244387,  0.20397018,\n",
       "        0.25838128,  0.4832477 , -0.22960119,  0.01627262, -0.21261746,\n",
       "       -0.2715558 ,  0.28467602,  0.15231182, -0.09285944,  0.28947443,\n",
       "       -0.06843787,  0.1255881 ,  0.8702048 ,  0.02227183,  0.27699545,\n",
       "        0.19527818,  0.38627282, -0.01307742, -0.12605608,  0.06644178,\n",
       "        0.02891991, -0.20725605,  0.17995903, -0.1606334 ,  0.27738062,\n",
       "       -0.44023108,  0.13876231, -0.17100142,  0.07651781, -0.31987593,\n",
       "        0.39109808, -0.64132386, -0.18748127, -0.15217304, -0.33366784,\n",
       "       -0.23702534, -0.09647428,  0.14715166,  0.50744605, -0.47037542,\n",
       "        0.22670864,  0.01208101, -0.5175762 ,  0.28740397,  0.24730249,\n",
       "       -0.14597584,  0.71664655, -0.10583282, -0.10196099, -0.26773652,\n",
       "       -0.30576602,  0.31484523, -0.33663896, -0.4439698 ,  0.13530843,\n",
       "       -0.48986742, -0.04291547,  0.04542742, -0.12595604,  0.01892297,\n",
       "       -0.16023673, -0.09223235, -0.3519353 , -0.27544725, -0.35941103,\n",
       "        0.26816747, -0.26366055,  0.09045722, -0.09209683, -0.19774392,\n",
       "        0.4626923 ,  0.11606661,  0.13637708,  0.0600035 ,  0.3952663 ,\n",
       "       -0.11322273,  0.12690416,  0.4223611 , -0.3319263 , -0.0074226 ,\n",
       "        0.181907  , -0.5575815 , -0.24921872,  0.5261132 , -0.17298555,\n",
       "        0.4189706 ,  0.00610934, -0.8102452 ,  0.13547905, -0.07609115,\n",
       "       -0.3848198 , -0.15416847,  0.46580663, -0.00973997,  0.41822353,\n",
       "       -0.0420084 ,  0.22273165, -0.6414529 ,  0.18634477, -0.1320177 ,\n",
       "       -0.3993432 , -0.29942054, -0.17588404,  0.01930285,  0.15464629,\n",
       "        0.0271406 ,  0.32141694,  0.26384956,  0.32256746, -0.0999136 ,\n",
       "       -0.00914712,  0.04311815,  0.06883949, -0.26403832, -0.23895177,\n",
       "        0.06563807, -0.32052973,  0.51667506,  0.28173164, -0.03728829,\n",
       "        0.10617348,  0.46699637, -0.15934104, -0.00895077,  0.03029792,\n",
       "       -0.0790227 , -0.15837127,  0.07571064,  0.08949807,  0.31985927,\n",
       "        0.22785452, -0.04943721, -0.39527982, -0.47163686,  0.20657642,\n",
       "        0.01421705,  0.4552891 , -0.13419099, -0.03937298, -0.12779118,\n",
       "        0.30422038,  0.00630774, -0.61479425,  0.30588475, -0.1631542 ,\n",
       "        0.47571513,  0.23350583, -0.03703165,  0.6147397 , -0.6354517 ,\n",
       "        0.04949358, -0.23769939, -0.0374764 , -0.20668483,  0.03984997,\n",
       "        0.05531057, -0.14646474, -0.7818065 , -0.1705449 , -0.47148925,\n",
       "        0.7534342 ,  0.16350834, -0.4402775 ,  0.3062885 , -0.2754479 ,\n",
       "        0.7068775 ,  0.74453825,  0.44501626, -0.32818314, -0.6157321 ,\n",
       "        0.24681088,  0.20351061, -0.4685785 , -0.20134608, -0.12309294],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navec[\"навек\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d824717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy. linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f762414",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=navec[\"чашка \"]\n",
    "b=navec[\"кувшин\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecbbbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = dot (a,b)/( norm (a) * norm (b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06801188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48653683\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
